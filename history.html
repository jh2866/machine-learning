<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU"
        crossorigin="anonymous" />
    <link rel="shortcut icon" type="image/svg" href="img/favicon_cleaned.svg" />
    <link rel="stylesheet" href="css/main.css" />
    <title>History</title>
</head>

<body>
    <header>
        <nav class="menu">
            <div class="menu-branding">
                <div class="portrait"></div>
            </div>
            <ul class="menu-nav">
                <li class="nav-item current">
                    <a href="index.html" class="nav-link">Overview</a>
                </li>
                <li class="nav-item">
                    <a href="approaches.html" class="nav-link">Approaches</a>
                </li>
                <li class="nav-item">
                    <a href="history.html" class="nav-link">History</a>
                </li>
                <li class="nav-item">
                    <a href="software.html" class="nav-link">Software</a>
                </li>
            </ul>
        </nav>
    </header>
    <main id="history">
        <h1 class="lg-heading">
            History</span>
        </h1>
        <p>Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term
        "Machine Learning" in 1959 while at IBM. As a scientific endeavour, machine learning grew out of the quest for
        artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in
        having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what
        were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be
        reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially
        in automated medical diagnosis.</p>
        <div class="about-info">
            <div class="job job-1">
                <h3 id="Central Square"></h3>
                <h4>Relation to data mining</h4>
                <p>
                    Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning
                    focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery
                    of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data
                    mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs
                    data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the
                    confusion between these two research communities (which do often have separate conferences and separate journals, ECML
                    PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is
                    usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data
                    mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge,
                    an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD
                    task, supervised methods cannot be used due to the unavailability of training data.

                </p>
            </div>
            <div class="job job-2">
                <h3 id="Front End"></h3>
                <h4>Rule-based machine learning</h4>
                <p>
                    Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some
                    loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the
                    model being trained and the actual problem instances (for example, in classification, one wants to assign a label to
                    instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference
                    between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a
                    training set, machine learning is concerned with minimizing the loss on unseen samples.
                </p>
            </div>
            <div class="job job-3">
                <h3 id="Graphics"></h3>
                <h4>Representation learning</h4>
                <p>
                    Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine
                    learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also
                    suggested the term data science as a placeholder to call the overall field.<br /><br />
                    
                    Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein
                    "algorithmic model" means more or less the machine learning algorithms like Random forest.<br /><br />
                    
                    Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical
                    learning.
                </p>
            </div>
        </div>
    </main>

    <footer id="main-footer">Copyright &copy; 2018</footer>

    <script src="js/main.js"></script>
</body>

</html>